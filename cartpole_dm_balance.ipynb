{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance taask in dm_control\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to make a NN learn the dm_control environement. As the action input of the cartpole environement is continous, a simple DQNAgent will not work and i'll first use a DDPG model with an actor and a critic\n",
    "\n",
    "## About DDPG\n",
    "\n",
    "DDPG (Deep Deterministic Policy Gradient) is used when dealing with continuous action spaces, while DQN (Deep Q-Network) is used for discrete action spaces. In problems with continuous action spaces, the number of possible actions is not finite, and actions can take any value within a certain range. DQN, which is based on estimating the Q-values for each action, is not well-suited for continuous action spaces because it would require evaluating the Q-value for an infinite number of actions. DDPG, on the other hand, directly learns a policy that maps states to actions, making it more suitable for continuous action spaces.\n",
    "\n",
    "DDPG is an off-policy, model-free algorithm that combines ideas from DQN and policy gradient methods. It uses two neural networks: an actor network and a critic network.\n",
    "\n",
    "**Actor**: The actor network learns a deterministic policy that maps states to actions. Given a state, the actor network directly outputs the best action to take in that state, according to the current policy. The actor network's goal is to maximize the expected return (the sum of rewards) following the policy.\n",
    "\n",
    "**Critic**: The critic network learns the Q-value function, which estimates the expected return for taking an action in a state following the current policy. The critic's goal is to learn an accurate estimate of the Q-values for state-action pairs. It is used to update the policy (actor network) by providing feedback on the quality of the chosen actions.\n",
    "The DDPG algorithm works as follows:\n",
    "\n",
    "1. Initialize the actor and critic networks, as well as their target networks (used to stabilize learning).\n",
    "2. At each time step, the agent takes an action based on the current policy (output of the actor network) and explores the environment using noise added to the actions.\n",
    "3. Store the observed transitions (state, action, reward, next_state, done) in a replay buffer.\n",
    "4. Randomly sample a batch of transitions from the replay buffer.\n",
    "5. Train the critic network using the sampled transitions and the target Q-values, which are calculated using the target networks.\n",
    "6. Train the actor network using the sampled transitions and the gradients of the Q-values concerning the actions. Update the policy to maximize the Q-values.\n",
    "7. Softly update the target networks (actor and critic) using a mix of the current and target network weights.\n",
    "By learning a policy directly through the actor network and using the critic network to improve the policy, DDPG can handle continuous action spaces efficiently.\n",
    "\n",
    "This text is copied from ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from dm_control import suite\n",
    "from dm_control import viewer\n",
    "from dm_env import StepType\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from statistics import mean, stdev\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = Dense(24, activation='relu')\n",
    "        self.fc2 = Dense(48, activation='relu')\n",
    "        self.fc3 = Dense(96, activation='relu')\n",
    "        self.fc4 = Dense(48, activation='relu')\n",
    "        self.fc5 = Dense(24, activation='relu')\n",
    "        self.fc6 = Dense(action_size, activation='tanh')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return self.fc6(x)\n",
    "\n",
    "class Critic(Model):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = Dense(24, activation='relu')\n",
    "        self.fc2 = Dense(48, activation='relu')\n",
    "        self.fc3 = Dense(96, activation='relu')\n",
    "        self.fc4 = Dense(48, activation='relu')\n",
    "        self.fc5 = Dense(24, activation='relu')\n",
    "        self.fc6 = Dense(1)\n",
    "\n",
    "    def call(self, state, action):\n",
    "        x = tf.concat([state, action], axis=-1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return self.fc6(x)\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, tau=0.005, lr_actor=1e-4, lr_critic=1e-3):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.actor = Actor(state_size, action_size)\n",
    "        self.actor_target = Actor(state_size, action_size)\n",
    "        self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_actor)\n",
    "\n",
    "        self.critic = Critic(state_size, action_size)\n",
    "        self.critic_target = Critic(state_size, action_size)\n",
    "        self.critic_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_critic)\n",
    "\n",
    "        self.noise_std_dev = 0.2\n",
    "        self.noise_clip = 0.5\n",
    "\n",
    "        self.update_target_networks(tau=1.0)  # Initialize the target networks\n",
    "\n",
    "    def update_target_networks(self, tau=None):\n",
    "        tau = self.tau if tau is None else tau\n",
    "\n",
    "        weights_actor = self.actor.get_weights()\n",
    "        weights_actor_target = self.actor_target.get_weights()\n",
    "        for i in range(len(weights_actor)):\n",
    "            weights_actor_target[i] = tau * weights_actor[i] + (1 - tau) * weights_actor_target[i]\n",
    "        self.actor_target.set_weights(weights_actor_target)\n",
    "\n",
    "        weights_critic = self.critic.get_weights()\n",
    "        weights_critic_target = self.critic_target.get_weights()\n",
    "        \n",
    "        for i in range(len(weights_critic)):\n",
    "            weights_critic_target[i] = tau * weights_critic[i] + (1 - tau) * weights_critic_target[i]\n",
    "        self.critic_target.set_weights(weights_critic_target)\n",
    "\n",
    "    def act(self, state, noise=True):\n",
    "        state = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "        action = self.actor(state).numpy()[0]\n",
    "\n",
    "        if noise:\n",
    "            noise = np.random.normal(loc=0, scale=self.noise_std_dev, size=self.action_size).clip(-self.noise_clip, self.noise_clip)\n",
    "            action = np.clip(action + noise, -1, 1)\n",
    "\n",
    "        return action\n",
    "    def train(self, replay_buffer, batch_size=64):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = replay_buffer.sample(batch_size)\n",
    "\n",
    "        # Train Critic\n",
    "        with tf.GradientTape() as tape:\n",
    "            next_actions = self.actor_target(next_state_batch)\n",
    "            target_q_values = self.critic_target(next_state_batch, next_actions)\n",
    "            y = reward_batch + (1 - done_batch) * self.gamma * target_q_values\n",
    "            q_values = self.critic(state_batch, action_batch)\n",
    "            critic_loss = tf.reduce_mean(tf.square(y - q_values))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.critic.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grad, self.critic.trainable_variables))\n",
    "\n",
    "        # Train Actor\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.actor(state_batch)\n",
    "            actor_loss = -tf.reduce_mean(self.critic(state_batch, actions))\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.actor.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grad, self.actor.trainable_variables))\n",
    "\n",
    "        # Update target networks\n",
    "        self.update_target_networks()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size=100000):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "action_size = env.action_spec().shape[0]\n",
    "action_low = env.action_spec().minimum[0]\n",
    "action_high = env.action_spec().maximum[0]\n",
    "states = env.observation_spec()\n",
    "state_size = states['position'].shape[0] + states['velocity'].shape[0]\n",
    "\n",
    "agent = DDPGAgent(state_size, action_size)\n",
    "replay_buffer = ReplayBuffer()\n",
    "\n",
    "num_episodes = 200\n",
    "max_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/10, Score: 74\n",
      "Test Episode: 2/10, Score: 75\n",
      "Test Episode: 3/10, Score: 88\n",
      "Test Episode: 4/10, Score: 62\n",
      "Test Episode: 5/10, Score: 98\n",
      "Test Episode: 6/10, Score: 99\n",
      "Test Episode: 7/10, Score: 76\n",
      "Test Episode: 8/10, Score: 97\n",
      "Test Episode: 9/10, Score: 87\n",
      "Test Episode: 10/10, Score: 95\n",
      "\n",
      "pretraining: score average  85.1\n"
     ]
    }
   ],
   "source": [
    "# pretraining test\n",
    "\n",
    "test_episodes = 10\n",
    "test_average = 0\n",
    "for e in range(test_episodes):\n",
    "    timestep, reward, discount, observation = env.reset()\n",
    "    state = np.concatenate((observation['position'],observation['velocity']))\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    t = 0\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = agent.act(state)\n",
    "        timestep, reward, discount, observation = env.step(action)\n",
    "        state = np.concatenate((observation['position'],observation['velocity']))\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        if timestep == StepType.LAST or state[0][1] < 0.9: # cos(24deg) = 0.9\n",
    "            t = _\n",
    "            break\n",
    "\n",
    "    print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "    test_average += t\n",
    "\n",
    "test_average/=test_episodes\n",
    "print()\n",
    "print('pretraining: score average ', test_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/200, Reward: 236.3871735683071\n",
      "Episode 2/200, Reward: 141.61476140906447\n",
      "Episode 3/200, Reward: 141.23328609274046\n",
      "Episode 4/200, Reward: 142.22228071592923\n",
      "Episode 5/200, Reward: 140.2418062902733\n",
      "Episode 6/200, Reward: 142.05827656501148\n",
      "Episode 7/200, Reward: 142.20981142773326\n",
      "Episode 8/200, Reward: 141.9345349872028\n",
      "Episode 9/200, Reward: 142.41715954032716\n",
      "Episode 10/200, Reward: 141.9734951733018\n",
      "Episode 11/200, Reward: 142.23837333320353\n",
      "Episode 12/200, Reward: 142.8993916525479\n",
      "Episode 13/200, Reward: 140.9960288319469\n",
      "Episode 14/200, Reward: 139.90230452343772\n",
      "Episode 15/200, Reward: 141.4556160026598\n",
      "Episode 16/200, Reward: 140.08364648995266\n",
      "Episode 17/200, Reward: 140.47708357572822\n",
      "Episode 18/200, Reward: 141.3656992479838\n",
      "Episode 19/200, Reward: 141.9421089401938\n",
      "Episode 20/200, Reward: 141.93559042106827\n",
      "Episode 21/200, Reward: 142.78570918159485\n",
      "Episode 22/200, Reward: 140.71775585973245\n",
      "Episode 23/200, Reward: 142.55179212421538\n",
      "Episode 24/200, Reward: 142.48578519904638\n",
      "Episode 25/200, Reward: 143.40459698970946\n",
      "Episode 26/200, Reward: 142.41644637670333\n",
      "Episode 27/200, Reward: 140.8359329517022\n",
      "Episode 28/200, Reward: 141.72524491901237\n",
      "Episode 29/200, Reward: 141.84871575178076\n",
      "Episode 30/200, Reward: 142.1212001079988\n",
      "Episode 31/200, Reward: 140.20139802296256\n",
      "Episode 32/200, Reward: 141.57495074955244\n",
      "Episode 33/200, Reward: 142.20471421240742\n",
      "Episode 34/200, Reward: 140.8090483680558\n",
      "Episode 35/200, Reward: 141.68485358023304\n",
      "Episode 36/200, Reward: 141.5057679473411\n",
      "Episode 37/200, Reward: 141.6539793915788\n",
      "Episode 38/200, Reward: 141.6095873571938\n",
      "Episode 39/200, Reward: 140.76605295007235\n",
      "Episode 40/200, Reward: 139.90251801341236\n",
      "Episode 41/200, Reward: 141.53621299133226\n",
      "Episode 42/200, Reward: 140.89252539846942\n",
      "Episode 43/200, Reward: 142.25133552348578\n",
      "Episode 44/200, Reward: 141.21078968721807\n",
      "Episode 45/200, Reward: 141.6255358441302\n",
      "Episode 46/200, Reward: 140.716465806913\n",
      "Episode 47/200, Reward: 140.5088594018201\n",
      "Episode 48/200, Reward: 141.16028611147084\n",
      "Episode 49/200, Reward: 143.46282708717004\n",
      "Episode 50/200, Reward: 142.02821184369535\n",
      "Episode 51/200, Reward: 141.92238990146492\n",
      "Episode 52/200, Reward: 140.85746025484343\n",
      "Episode 53/200, Reward: 140.45017021575381\n",
      "Episode 54/200, Reward: 140.41326687047993\n",
      "Episode 55/200, Reward: 141.5562077867093\n",
      "Episode 56/200, Reward: 141.049556453212\n",
      "Episode 57/200, Reward: 142.66598812350946\n",
      "Episode 58/200, Reward: 141.06510560783332\n",
      "Episode 59/200, Reward: 141.00788052149642\n",
      "Episode 60/200, Reward: 142.1112416517377\n",
      "Episode 61/200, Reward: 141.52901552669778\n",
      "Episode 62/200, Reward: 140.77589033836338\n",
      "Episode 63/200, Reward: 140.6093590949785\n",
      "Episode 64/200, Reward: 141.86713493825158\n",
      "Episode 65/200, Reward: 140.774527128809\n",
      "Episode 66/200, Reward: 142.66798779015582\n",
      "Episode 67/200, Reward: 141.86920958765543\n",
      "Episode 68/200, Reward: 141.87125501796848\n",
      "Episode 69/200, Reward: 142.0623589698545\n",
      "Episode 70/200, Reward: 141.0270655485313\n",
      "Episode 71/200, Reward: 142.24501445260938\n",
      "Episode 72/200, Reward: 142.9369550118791\n",
      "Episode 73/200, Reward: 142.27851288753493\n",
      "Episode 74/200, Reward: 141.43488658597178\n",
      "Episode 75/200, Reward: 141.94539437333083\n",
      "Episode 76/200, Reward: 143.25377686144265\n",
      "Episode 77/200, Reward: 139.75304434889193\n",
      "Episode 78/200, Reward: 140.39339742501622\n",
      "Episode 79/200, Reward: 141.85374739877574\n",
      "Episode 80/200, Reward: 142.40267239816586\n",
      "Episode 81/200, Reward: 141.78025559365707\n",
      "Episode 82/200, Reward: 142.1365580488688\n",
      "Episode 83/200, Reward: 142.74928197822223\n",
      "Episode 84/200, Reward: 141.95347503154358\n",
      "Episode 85/200, Reward: 141.23417031698733\n",
      "Episode 86/200, Reward: 140.50011320944597\n",
      "Episode 87/200, Reward: 143.04860261205016\n",
      "Episode 88/200, Reward: 142.4863948736155\n",
      "Episode 89/200, Reward: 142.58469699629882\n",
      "Episode 90/200, Reward: 142.55188393629263\n",
      "Episode 91/200, Reward: 142.67928556531527\n",
      "Episode 92/200, Reward: 142.06456934910796\n",
      "Episode 93/200, Reward: 140.86539633848227\n",
      "Episode 94/200, Reward: 140.77221805972093\n",
      "Episode 95/200, Reward: 140.988678940679\n",
      "Episode 96/200, Reward: 141.39011526156986\n",
      "Episode 97/200, Reward: 141.02682442282392\n",
      "Episode 98/200, Reward: 141.93474386897705\n",
      "Episode 99/200, Reward: 141.79672725927873\n",
      "Episode 100/200, Reward: 142.7946168430091\n",
      "Episode 101/200, Reward: 141.5463557411155\n",
      "Episode 102/200, Reward: 143.0149309612031\n",
      "Episode 103/200, Reward: 141.38311223782767\n",
      "Episode 104/200, Reward: 141.0056742273742\n",
      "Episode 105/200, Reward: 141.02489626562584\n",
      "Episode 106/200, Reward: 141.88509778010012\n",
      "Episode 107/200, Reward: 142.8964179270587\n",
      "Episode 108/200, Reward: 141.23101591428068\n",
      "Episode 109/200, Reward: 140.7290239752559\n",
      "Episode 110/200, Reward: 141.9541905086045\n",
      "Episode 111/200, Reward: 141.4999688482039\n",
      "Episode 112/200, Reward: 140.22033432644847\n",
      "Episode 113/200, Reward: 141.70565308702788\n",
      "Episode 114/200, Reward: 140.92734590747668\n",
      "Episode 115/200, Reward: 142.8466858480564\n",
      "Episode 116/200, Reward: 142.89675284209142\n",
      "Episode 117/200, Reward: 140.34441369033345\n",
      "Episode 118/200, Reward: 140.41980426866695\n",
      "Episode 119/200, Reward: 140.83318069141527\n",
      "Episode 120/200, Reward: 140.02595215272441\n",
      "Episode 121/200, Reward: 141.53963137542445\n",
      "Episode 122/200, Reward: 142.49504299147762\n",
      "Episode 123/200, Reward: 141.57443165872056\n",
      "Episode 124/200, Reward: 141.35490752565164\n",
      "Episode 125/200, Reward: 141.99476879826318\n",
      "Episode 126/200, Reward: 141.70821629771464\n",
      "Episode 127/200, Reward: 140.37038707155932\n",
      "Episode 128/200, Reward: 141.62221412910213\n",
      "Episode 129/200, Reward: 142.15040176139468\n",
      "Episode 130/200, Reward: 141.42017218405675\n",
      "Episode 131/200, Reward: 142.35419629448197\n",
      "Episode 132/200, Reward: 142.6820551123098\n",
      "Episode 133/200, Reward: 140.5765414479592\n",
      "Episode 134/200, Reward: 139.5130151783913\n",
      "Episode 135/200, Reward: 141.69692001137204\n",
      "Episode 136/200, Reward: 141.07558590959513\n",
      "Episode 137/200, Reward: 142.94204565388108\n",
      "Episode 138/200, Reward: 142.3769129921193\n",
      "Episode 139/200, Reward: 143.0245800519318\n",
      "Episode 140/200, Reward: 141.18573424765495\n",
      "Episode 141/200, Reward: 140.11191860278888\n",
      "Episode 142/200, Reward: 142.60112633932187\n",
      "Episode 143/200, Reward: 143.41779269174603\n",
      "Episode 144/200, Reward: 143.0695008113163\n",
      "Episode 145/200, Reward: 143.02237771553155\n",
      "Episode 146/200, Reward: 143.52827069600554\n",
      "Episode 147/200, Reward: 141.5749750381858\n",
      "Episode 148/200, Reward: 139.95606314869505\n",
      "Episode 149/200, Reward: 139.99905800436295\n",
      "Episode 150/200, Reward: 139.43278108275015\n",
      "Episode 151/200, Reward: 142.5529990166829\n",
      "Episode 152/200, Reward: 143.5291575909497\n",
      "Episode 153/200, Reward: 141.03121737419735\n",
      "Episode 154/200, Reward: 142.06544504957182\n",
      "Episode 155/200, Reward: 140.92915233858386\n",
      "Episode 156/200, Reward: 141.35822741459725\n",
      "Episode 157/200, Reward: 140.48481622165627\n",
      "Episode 158/200, Reward: 142.24068839962473\n",
      "Episode 159/200, Reward: 141.52691962939147\n",
      "Episode 160/200, Reward: 141.79617947940702\n",
      "Episode 161/200, Reward: 142.61597959572688\n",
      "Episode 162/200, Reward: 142.44875130402315\n",
      "Episode 163/200, Reward: 140.9963765468955\n",
      "Episode 164/200, Reward: 142.76558109752096\n",
      "Episode 165/200, Reward: 141.89822557996075\n",
      "Episode 166/200, Reward: 142.5512288759301\n",
      "Episode 167/200, Reward: 143.31375354707615\n",
      "Episode 168/200, Reward: 140.58503223658252\n",
      "Episode 169/200, Reward: 142.86887706952265\n",
      "Episode 170/200, Reward: 140.759549107681\n",
      "Episode 171/200, Reward: 141.77319448983621\n",
      "Episode 172/200, Reward: 140.46424491274664\n",
      "Episode 173/200, Reward: 141.1802076767034\n",
      "Episode 174/200, Reward: 143.04939847364713\n",
      "Episode 175/200, Reward: 140.332338032764\n",
      "Episode 176/200, Reward: 142.17481766495962\n",
      "Episode 177/200, Reward: 140.7640976636237\n",
      "Episode 178/200, Reward: 140.72849997379038\n",
      "Episode 179/200, Reward: 141.8545885002779\n",
      "Episode 180/200, Reward: 142.78483096918953\n",
      "Episode 181/200, Reward: 142.26390370666255\n",
      "Episode 182/200, Reward: 141.55113100499707\n",
      "Episode 183/200, Reward: 143.27912935499623\n",
      "Episode 184/200, Reward: 142.33644937213097\n",
      "Episode 185/200, Reward: 140.16882899280083\n",
      "Episode 186/200, Reward: 141.59803983304596\n",
      "Episode 187/200, Reward: 142.86536111542244\n",
      "Episode 188/200, Reward: 140.4398714568253\n",
      "Episode 189/200, Reward: 141.155670647741\n",
      "Episode 190/200, Reward: 142.40735606323923\n",
      "Episode 191/200, Reward: 141.29917912708433\n",
      "Episode 192/200, Reward: 142.09112071879727\n",
      "Episode 193/200, Reward: 141.42201826310605\n",
      "Episode 194/200, Reward: 141.91515207645125\n",
      "Episode 195/200, Reward: 140.86571000420486\n",
      "Episode 196/200, Reward: 141.53155965213261\n",
      "Episode 197/200, Reward: 141.0612615682327\n",
      "Episode 198/200, Reward: 141.9348330355678\n",
      "Episode 199/200, Reward: 141.16822298752217\n",
      "Episode 200/200, Reward: 141.44453918182109\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for episode in range(num_episodes):\n",
    "    timestep, reward, discount, observation = env.reset()\n",
    "    state = np.concatenate((observation['position'],observation['velocity']))\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    episode_reward = 0\n",
    "\n",
    "    for step in range(max_steps):\n",
    "        action = agent.act(state)\n",
    "        timestep, reward, discount, observation = env.step(action)\n",
    "        next_state = np.concatenate((observation['position'],observation['velocity']))\n",
    "        next_state = np.reshape(state, [1, state_size])\n",
    "\n",
    "        if timestep == StepType.MID:\n",
    "            done = 0\n",
    "        elif timestep == StepType.LAST:\n",
    "            done = 1\n",
    "\n",
    "        replay_buffer.add(state, action, reward, next_state, float(done))\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if len(replay_buffer) >= 64:\n",
    "            agent.train(replay_buffer)\n",
    "\n",
    "        if done or state[0][1] < 0.9:\n",
    "            break\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Reward: {episode_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/30, Score: 16\n",
      "Test Episode: 2/30, Score: 19\n",
      "Test Episode: 3/30, Score: 16\n",
      "Test Episode: 4/30, Score: 15\n",
      "Test Episode: 5/30, Score: 17\n",
      "Test Episode: 6/30, Score: 19\n",
      "Test Episode: 7/30, Score: 16\n",
      "Test Episode: 8/30, Score: 16\n",
      "Test Episode: 9/30, Score: 18\n",
      "Test Episode: 10/30, Score: 17\n",
      "Test Episode: 11/30, Score: 16\n",
      "Test Episode: 12/30, Score: 16\n",
      "Test Episode: 13/30, Score: 16\n",
      "Test Episode: 14/30, Score: 19\n",
      "Test Episode: 15/30, Score: 15\n",
      "Test Episode: 16/30, Score: 17\n",
      "Test Episode: 17/30, Score: 16\n",
      "Test Episode: 18/30, Score: 17\n",
      "Test Episode: 19/30, Score: 18\n",
      "Test Episode: 20/30, Score: 17\n",
      "Test Episode: 21/30, Score: 17\n",
      "Test Episode: 22/30, Score: 16\n",
      "Test Episode: 23/30, Score: 17\n",
      "Test Episode: 24/30, Score: 17\n",
      "Test Episode: 25/30, Score: 16\n",
      "Test Episode: 26/30, Score: 18\n",
      "Test Episode: 27/30, Score: 19\n",
      "Test Episode: 28/30, Score: 17\n",
      "Test Episode: 29/30, Score: 17\n",
      "Test Episode: 30/30, Score: 18\n",
      "\n",
      "Score average: 16.93, Sigma: 1.14\n",
      "Average time per step: 0.0019 seconds\n",
      "Reward average: 13.55, Sigma: 0.88\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    time_step = env.reset()\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(1000): # 1000 steps (half delta t of gym)\n",
    "        action = agent.act(state)\n",
    "        timestep, reward, discount, observation = env.step(action)\n",
    "        state = np.concatenate((observation['position'],observation['velocity']))\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        sum_rewards += reward\n",
    "    \n",
    "\n",
    "        if observation['position'][1] < 0.978 or timestep == StepType.LAST or t == 999 : # cos of angle < cos of 12 degrees--> angle > 12deg or 1000 steps (half delta t of gym) \n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_scores.append(t)\n",
    "            test_rewards.append(sum_rewards)\n",
    "            break\n",
    "\n",
    "test_average = mean(test_scores)\n",
    "test_sigma = stdev(test_scores)\n",
    "reward_average = mean(test_rewards)\n",
    "reward_sigma = stdev(test_rewards)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / total_steps\n",
    "\n",
    "print()\n",
    "print('Score average: {:.2f}, Sigma: {:.2f}'.format(test_average, test_sigma))\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "print('Reward average: {:.2f}, Sigma: {:.2f}'.format(reward_average, reward_sigma))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video video/DDPG_dm_balance.mp4.\n",
      "Moviepy - Writing video video/DDPG_dm_balance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video/DDPG_dm_balance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#create video\n",
    "\n",
    "def ddpg_policy(timestep):\n",
    "    state = np.concatenate((timestep.observation['position'],timestep.observation['velocity']))\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    action = agent.act(state)\n",
    "    return action\n",
    "    \n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Load the cartpole environment\n",
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "\n",
    "# Visualization and video creation\n",
    "def save_video(policy):\n",
    "    frames = []\n",
    "\n",
    "    def policy_with_frame_grab(time_step):\n",
    "        pixels = env.physics.render(height=480, width=640, camera_id=0)\n",
    "        frames.append(pixels)\n",
    "        return policy(time_step)\n",
    "\n",
    "    # Create the viewer application\n",
    "    viewer.launch(env, policy=policy_with_frame_grab)\n",
    "\n",
    "    # Save the frames as a video\n",
    "    clip = ImageSequenceClip(frames, fps=100)\n",
    "    clip.write_videofile(\"video/DDPG_dm_balance.mp4\", codec=\"libx264\")\n",
    "\n",
    "# Call the save_video function with your policy function\n",
    "save_video(ddpg_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "env.step(1)\n",
    "obs_shape = sum([value.shape[0] for value in env.observation_spec().values()])\n",
    "obs_shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPG with Stablebaselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dm_control import suite\n",
    "#from dm_control.rl.wrappers import pixels\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from stable_baselines3 import DDPG\n",
    "\n",
    "class DMControlWrapper(gym.Env):\n",
    "    def __init__(self, domain_name, task_name):\n",
    "        self.env = suite.load(domain_name=domain_name, task_name=task_name)\n",
    "        obs_shape = sum([value.shape[0] for value in self.env.observation_spec().values()])\n",
    "        self.observation_space = Box(low=-np.inf, high=np.inf, shape=(obs_shape,), dtype=np.float32)\n",
    "        self.action_space = Box(low=self.env.action_spec().minimum[0], high=self.env.action_spec().maximum[0], shape=self.env.action_spec().shape, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        time_step = self.env.reset()\n",
    "        return np.array(self.get_obs(time_step))\n",
    "\n",
    "    def step(self, action):\n",
    "        time_step = self.env.step(action)\n",
    "        return np.array(self.get_obs(time_step)), time_step.reward, time_step.last(), {}\n",
    "\n",
    "    def get_obs(self, time_step):\n",
    "        return np.concatenate([value for value in time_step.observation.values()])\n",
    "    \n",
    "    def action_spec(self):\n",
    "        return self.env.action_spec()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.03781814,  0.99992663,  0.01211301,  0.10266851, -0.15240272]),\n",
       " 0.7987870816984224,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = DMControlWrapper(\"cartpole\", \"balance\")\n",
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = DMControlWrapper(\"cartpole\", \"balance\")\n",
    "DDPG_model = DDPG(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 173      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 103      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.39    |\n",
      "|    critic_loss     | 0.00291  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 174      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 88       |\n",
      "|    time_elapsed    | 90       |\n",
      "|    total timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -11      |\n",
      "|    critic_loss     | 0.158    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 205      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 41       |\n",
      "|    time_elapsed    | 289      |\n",
      "|    total timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -18.9    |\n",
      "|    critic_loss     | 0.434    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 218      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 21       |\n",
      "|    time_elapsed    | 755      |\n",
      "|    total timesteps | 16000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -26.6    |\n",
      "|    critic_loss     | 0.876    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 15000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 263      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 16       |\n",
      "|    time_elapsed    | 1197     |\n",
      "|    total timesteps | 20000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -28.3    |\n",
      "|    critic_loss     | 0.609    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 19000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 287      |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 14       |\n",
      "|    time_elapsed    | 1645     |\n",
      "|    total timesteps | 24000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -30.4    |\n",
      "|    critic_loss     | 0.902    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 317      |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 2098     |\n",
      "|    total timesteps | 28000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -33.2    |\n",
      "|    critic_loss     | 0.83     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 27000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 355      |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 2595     |\n",
      "|    total timesteps | 32000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -36.6    |\n",
      "|    critic_loss     | 1.04     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 31000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 381      |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 3019     |\n",
      "|    total timesteps | 36000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -39.7    |\n",
      "|    critic_loss     | 1.23     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 35000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 417      |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 3430     |\n",
      "|    total timesteps | 40000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -45      |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 39000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 440      |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 3824     |\n",
      "|    total timesteps | 44000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -48.7    |\n",
      "|    critic_loss     | 1.94     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 43000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 459      |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 4230     |\n",
      "|    total timesteps | 48000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -53.3    |\n",
      "|    critic_loss     | 2.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 47000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 471      |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 4626     |\n",
      "|    total timesteps | 52000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -56.7    |\n",
      "|    critic_loss     | 3.01     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 51000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 477      |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 5048     |\n",
      "|    total timesteps | 56000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -59.3    |\n",
      "|    critic_loss     | 3.29     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 55000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 480      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 5449     |\n",
      "|    total timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -61.4    |\n",
      "|    critic_loss     | 2.39     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 59000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 496      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 10       |\n",
      "|    time_elapsed    | 5854     |\n",
      "|    total timesteps | 64000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -63.9    |\n",
      "|    critic_loss     | 3.97     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 63000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 507      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 10       |\n",
      "|    time_elapsed    | 6278     |\n",
      "|    total timesteps | 68000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -66      |\n",
      "|    critic_loss     | 3.84     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 67000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 527      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 10       |\n",
      "|    time_elapsed    | 6681     |\n",
      "|    total timesteps | 72000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -69.3    |\n",
      "|    critic_loss     | 4.46     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 71000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 542      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 10       |\n",
      "|    time_elapsed    | 6938     |\n",
      "|    total timesteps | 76000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -72      |\n",
      "|    critic_loss     | 5.7      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 75000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 559      |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 6990     |\n",
      "|    total timesteps | 80000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -75.3    |\n",
      "|    critic_loss     | 5.28     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 79000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 579      |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 11       |\n",
      "|    time_elapsed    | 7035     |\n",
      "|    total timesteps | 84000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -78.2    |\n",
      "|    critic_loss     | 5.22     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 83000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 597      |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 7081     |\n",
      "|    total timesteps | 88000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -79.7    |\n",
      "|    critic_loss     | 6.77     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 87000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 614      |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 12       |\n",
      "|    time_elapsed    | 7127     |\n",
      "|    total timesteps | 92000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -81.2    |\n",
      "|    critic_loss     | 6.03     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 91000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 629      |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 7177     |\n",
      "|    total timesteps | 96000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -82.5    |\n",
      "|    critic_loss     | 5.84     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 95000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 639      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 13       |\n",
      "|    time_elapsed    | 7236     |\n",
      "|    total timesteps | 100000   |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -83.4    |\n",
      "|    critic_loss     | 7.4      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 99000    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ddpg.ddpg.DDPG at 0x29cb5b1f0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DDPG_model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.34260428\n"
     ]
    }
   ],
   "source": [
    "print(DDPG_model.predict(observation)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/30, Score: 519\n",
      "Test Episode: 2/30, Score: 524\n",
      "Test Episode: 3/30, Score: 525\n",
      "Test Episode: 4/30, Score: 573\n",
      "Test Episode: 5/30, Score: 508\n",
      "Test Episode: 6/30, Score: 609\n",
      "Test Episode: 7/30, Score: 611\n",
      "Test Episode: 8/30, Score: 497\n",
      "Test Episode: 9/30, Score: 484\n",
      "Test Episode: 10/30, Score: 452\n",
      "Test Episode: 11/30, Score: 598\n",
      "Test Episode: 12/30, Score: 582\n",
      "Test Episode: 13/30, Score: 565\n",
      "Test Episode: 14/30, Score: 556\n",
      "Test Episode: 15/30, Score: 510\n",
      "Test Episode: 16/30, Score: 592\n",
      "Test Episode: 17/30, Score: 515\n",
      "Test Episode: 18/30, Score: 565\n",
      "Test Episode: 19/30, Score: 573\n",
      "Test Episode: 20/30, Score: 542\n",
      "Test Episode: 21/30, Score: 584\n",
      "Test Episode: 22/30, Score: 536\n",
      "Test Episode: 23/30, Score: 530\n",
      "Test Episode: 24/30, Score: 604\n",
      "Test Episode: 25/30, Score: 553\n",
      "Test Episode: 26/30, Score: 465\n",
      "Test Episode: 27/30, Score: 514\n",
      "Test Episode: 28/30, Score: 511\n",
      "Test Episode: 29/30, Score: 539\n",
      "Test Episode: 30/30, Score: 492\n",
      "\n",
      "Score average: 540.93, Sigma: 42.85\n",
      "Average time per step: 0.0002 seconds\n",
      "Reward average: 497.32, Sigma: 42.15\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "env = DMControlWrapper(\"cartpole\", \"balance\")\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(1000): # 1000 steps (half delta t of gym)\n",
    "        action = DDPG_model.predict(state)[0][0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        sum_rewards += reward\n",
    "    \n",
    "\n",
    "        if state[1] < 0.978 or done or t == 999 : # cos of angle < cos of 12 degrees--> angle > 12deg or 1000 steps (half delta t of gym) \n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_scores.append(t)\n",
    "            test_rewards.append(sum_rewards)\n",
    "            break\n",
    "\n",
    "test_average = mean(test_scores)\n",
    "test_sigma = stdev(test_scores)\n",
    "reward_average = mean(test_rewards)\n",
    "reward_sigma = stdev(test_rewards)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / total_steps\n",
    "\n",
    "print()\n",
    "print('Score average: {:.2f}, Sigma: {:.2f}'.format(test_average, test_sigma))\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "print('Reward average: {:.2f}, Sigma: {:.2f}'.format(reward_average, reward_sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video video/DDPG_sb3_dm_balance.mp4.\n",
      "Moviepy - Writing video video/DDPG_sb3_dm_balance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video/DDPG_sb3_dm_balance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#create video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "def ddpg_policy(time_step):\n",
    "    timestep, reward, discount, observation = time_step\n",
    "    state = np.concatenate((observation['position'],observation['velocity']))\n",
    "    action = DDPG_model.predict(state)[0][0]\n",
    "    return action\n",
    "\n",
    "# Load the cartpole environment\n",
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "\n",
    "# Visualization and video creation\n",
    "def save_video(policy):\n",
    "    frames = []\n",
    "\n",
    "    def policy_with_frame_grab(time_step):\n",
    "        pixels = env.physics.render(height=480, width=640, camera_id=0)\n",
    "        frames.append(pixels)\n",
    "        return policy(time_step)\n",
    "\n",
    "    # Create the viewer application\n",
    "    viewer.launch(env, policy=policy_with_frame_grab)\n",
    "\n",
    "    # Save the frames as a video\n",
    "    clip = ImageSequenceClip(frames, fps=100)\n",
    "    clip.write_videofile(\"video/DDPG_sb3_dm_balance.mp4\", codec=\"libx264\")\n",
    "\n",
    "# Call the save_video function with your policy function\n",
    "save_video(ddpg_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With PPO from Stablebaselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 343      |\n",
      "| time/              |          |\n",
      "|    fps             | 3655     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 304          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2384         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 1            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024196494 |\n",
      "|    clip_fraction        | 0.00376      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.0348       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4            |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.000396    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 23.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 297          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2153         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 2            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039040828 |\n",
      "|    clip_fraction        | 0.0246       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.615        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.09         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 10.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 304          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2092         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025222506 |\n",
      "|    clip_fraction        | 0.0265       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.42        |\n",
      "|    explained_variance   | 0.747        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.905        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00292     |\n",
      "|    std                  | 0.985        |\n",
      "|    value_loss           | 6.13         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 305          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2066         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046763746 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    std                  | 0.977        |\n",
      "|    value_loss           | 7.27         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 307         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2054        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002917219 |\n",
      "|    clip_fraction        | 0.00674     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.872       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.04        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    std                  | 0.969       |\n",
      "|    value_loss           | 6.62        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 316         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2041        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003368829 |\n",
      "|    clip_fraction        | 0.0264      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.965       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    std                  | 0.954       |\n",
      "|    value_loss           | 5.57        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 329          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1926         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050256494 |\n",
      "|    clip_fraction        | 0.0287       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.893        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00249     |\n",
      "|    std                  | 0.949        |\n",
      "|    value_loss           | 7.41         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 1e+03     |\n",
      "|    ep_rew_mean          | 323       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1890      |\n",
      "|    iterations           | 9         |\n",
      "|    time_elapsed         | 9         |\n",
      "|    total_timesteps      | 18432     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0041989 |\n",
      "|    clip_fraction        | 0.0142    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.36     |\n",
      "|    explained_variance   | 0.868     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.46      |\n",
      "|    n_updates            | 80        |\n",
      "|    policy_gradient_loss | -0.000329 |\n",
      "|    std                  | 0.947     |\n",
      "|    value_loss           | 7.65      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 331          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1822         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054059206 |\n",
      "|    clip_fraction        | 0.0461       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.885        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.754        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00543     |\n",
      "|    std                  | 0.948        |\n",
      "|    value_loss           | 8.88         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 331          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1780         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 22528        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064436197 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.905        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.49         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    std                  | 0.962        |\n",
      "|    value_loss           | 7.68         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 336          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1738         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032196012 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.883        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00214     |\n",
      "|    std                  | 0.96         |\n",
      "|    value_loss           | 7.22         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 339        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1726       |\n",
      "|    iterations           | 13         |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 26624      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00469902 |\n",
      "|    clip_fraction        | 0.0327     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.918      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.73       |\n",
      "|    n_updates            | 120        |\n",
      "|    policy_gradient_loss | -0.00297   |\n",
      "|    std                  | 0.96       |\n",
      "|    value_loss           | 7.55       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 341          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1711         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 28672        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056643737 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.925        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06         |\n",
      "|    n_updates            | 130          |\n",
      "|    policy_gradient_loss | -0.00307     |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 5.36         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1693        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005141286 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.941       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.589       |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00104    |\n",
      "|    std                  | 0.955       |\n",
      "|    value_loss           | 6.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 341         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1685        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006000838 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.946       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.805       |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00244    |\n",
      "|    std                  | 0.946       |\n",
      "|    value_loss           | 3.77        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 345          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1670         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026838088 |\n",
      "|    clip_fraction        | 0.0132       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.37        |\n",
      "|    explained_variance   | 0.942        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.000965    |\n",
      "|    std                  | 0.953        |\n",
      "|    value_loss           | 8.77         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 345         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1654        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005066709 |\n",
      "|    clip_fraction        | 0.032       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.37       |\n",
      "|    explained_variance   | 0.966       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.975       |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    std                  | 0.948       |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 347          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1644         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040672463 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.36        |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.1          |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00208     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 3.85         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 344          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1635         |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044501894 |\n",
      "|    clip_fraction        | 0.0285       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.35        |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.589        |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.00265     |\n",
      "|    std                  | 0.936        |\n",
      "|    value_loss           | 3.42         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 352         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1629        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003649843 |\n",
      "|    clip_fraction        | 0.0294      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00418    |\n",
      "|    std                  | 0.933       |\n",
      "|    value_loss           | 4.92        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 347         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1629        |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 27          |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003971616 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.948       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.86        |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    std                  | 0.935       |\n",
      "|    value_loss           | 8.53        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 344          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1617         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041873734 |\n",
      "|    clip_fraction        | 0.0372       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.97         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.44         |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    std                  | 0.92         |\n",
      "|    value_loss           | 2.65         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 342          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1607         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044666794 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.33        |\n",
      "|    explained_variance   | 0.977        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.71         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.0033      |\n",
      "|    std                  | 0.916        |\n",
      "|    value_loss           | 3.39         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 341          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1601         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073176166 |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.568        |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00593     |\n",
      "|    std                  | 0.902        |\n",
      "|    value_loss           | 2.38         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 341          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1594         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046312604 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.31        |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.474        |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00269     |\n",
      "|    std                  | 0.888        |\n",
      "|    value_loss           | 3.79         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 348          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1591         |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 55296        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046621757 |\n",
      "|    clip_fraction        | 0.0389       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.29        |\n",
      "|    explained_variance   | 0.98         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.284        |\n",
      "|    n_updates            | 260          |\n",
      "|    policy_gradient_loss | -0.0034      |\n",
      "|    std                  | 0.869        |\n",
      "|    value_loss           | 3.39         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 353          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1588         |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 57344        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056543685 |\n",
      "|    clip_fraction        | 0.0512       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.961        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14         |\n",
      "|    n_updates            | 270          |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    std                  | 0.872        |\n",
      "|    value_loss           | 4.9          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 362          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1577         |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 37           |\n",
      "|    total_timesteps      | 59392        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020983866 |\n",
      "|    clip_fraction        | 0.0163       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.962        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 280          |\n",
      "|    policy_gradient_loss | -0.00302     |\n",
      "|    std                  | 0.863        |\n",
      "|    value_loss           | 4.92         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 368         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1574        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006427031 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.985       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.18        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.00711    |\n",
      "|    std                  | 0.867       |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 372         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1576        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008294539 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.979       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.684       |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00636    |\n",
      "|    std                  | 0.842       |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 374          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1572         |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040405216 |\n",
      "|    clip_fraction        | 0.0476       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | 0.949        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.44         |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.00558     |\n",
      "|    std                  | 0.838        |\n",
      "|    value_loss           | 7.7          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 382         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1570        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004368095 |\n",
      "|    clip_fraction        | 0.0478      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.251       |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00214    |\n",
      "|    std                  | 0.828       |\n",
      "|    value_loss           | 2.6         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 396          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1569         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 44           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053938422 |\n",
      "|    clip_fraction        | 0.0725       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.989        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.335        |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00901     |\n",
      "|    std                  | 0.821        |\n",
      "|    value_loss           | 2.77         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 409          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1569         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 45           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0023953458 |\n",
      "|    clip_fraction        | 0.0887       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.69         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.92         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    std                  | 0.818        |\n",
      "|    value_loss           | 28.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 421         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1566        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004896614 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.918       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.55        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.00203    |\n",
      "|    std                  | 0.802       |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 427         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1565        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008798782 |\n",
      "|    clip_fraction        | 0.1         |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.867       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.5        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00387    |\n",
      "|    std                  | 0.796       |\n",
      "|    value_loss           | 23.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 435         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1562        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011454635 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.349       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    std                  | 0.767       |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 446         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1560        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006756384 |\n",
      "|    clip_fraction        | 0.0737      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.931       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    std                  | 0.763       |\n",
      "|    value_loss           | 20.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 457          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1559         |\n",
      "|    iterations           | 40           |\n",
      "|    time_elapsed         | 52           |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043500513 |\n",
      "|    clip_fraction        | 0.0254       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.839        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.1         |\n",
      "|    n_updates            | 390          |\n",
      "|    policy_gradient_loss | -0.00114     |\n",
      "|    std                  | 0.753        |\n",
      "|    value_loss           | 23.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 466         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1554        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006476336 |\n",
      "|    clip_fraction        | 0.0341      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.12       |\n",
      "|    explained_variance   | 0.789       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.34        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    std                  | 0.736       |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 480          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1557         |\n",
      "|    iterations           | 42           |\n",
      "|    time_elapsed         | 55           |\n",
      "|    total_timesteps      | 86016        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0079509355 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.806        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.16         |\n",
      "|    n_updates            | 410          |\n",
      "|    policy_gradient_loss | -0.0042      |\n",
      "|    std                  | 0.734        |\n",
      "|    value_loss           | 23.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 487         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1553        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004830839 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | 0.937       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.55        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    std                  | 0.724       |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 496         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1552        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 58          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013137447 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.984       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.796       |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    std                  | 0.7         |\n",
      "|    value_loss           | 5.08        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 504         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1548        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007640075 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.92        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.79        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    std                  | 0.696       |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 512         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1542        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011471531 |\n",
      "|    clip_fraction        | 0.142       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.874       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.61        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00814    |\n",
      "|    std                  | 0.694       |\n",
      "|    value_loss           | 22.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 1e+03       |\n",
      "|    ep_rew_mean          | 520         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1541        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013846848 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.08        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00837    |\n",
      "|    std                  | 0.677       |\n",
      "|    value_loss           | 22.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 1e+03        |\n",
      "|    ep_rew_mean          | 527          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1538         |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061147627 |\n",
      "|    clip_fraction        | 0.0625       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | 0.919        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.0022      |\n",
      "|    std                  | 0.657        |\n",
      "|    value_loss           | 21.8         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 1e+03      |\n",
      "|    ep_rew_mean          | 532        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1536       |\n",
      "|    iterations           | 49         |\n",
      "|    time_elapsed         | 65         |\n",
      "|    total_timesteps      | 100352     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01083504 |\n",
      "|    clip_fraction        | 0.0408     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.991     |\n",
      "|    explained_variance   | 0.474      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.06       |\n",
      "|    n_updates            | 480        |\n",
      "|    policy_gradient_loss | -0.00172   |\n",
      "|    std                  | 0.647      |\n",
      "|    value_loss           | 50.4       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2b279b7c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "env = DMControlWrapper(\"cartpole\", \"balance\")\n",
    "PPO_model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "PPO_model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy network architecture: ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# information about the model\n",
    "print(\"Policy network architecture:\", PPO_model.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/30, Score: 999\n",
      "Test Episode: 2/30, Score: 999\n",
      "Test Episode: 3/30, Score: 999\n",
      "Test Episode: 4/30, Score: 999\n",
      "Test Episode: 5/30, Score: 999\n",
      "Test Episode: 6/30, Score: 999\n",
      "Test Episode: 7/30, Score: 999\n",
      "Test Episode: 8/30, Score: 999\n",
      "Test Episode: 9/30, Score: 999\n",
      "Test Episode: 10/30, Score: 999\n",
      "Test Episode: 11/30, Score: 999\n",
      "Test Episode: 12/30, Score: 999\n",
      "Test Episode: 13/30, Score: 999\n",
      "Test Episode: 14/30, Score: 999\n",
      "Test Episode: 15/30, Score: 999\n",
      "Test Episode: 16/30, Score: 999\n",
      "Test Episode: 17/30, Score: 999\n",
      "Test Episode: 18/30, Score: 999\n",
      "Test Episode: 19/30, Score: 999\n",
      "Test Episode: 20/30, Score: 999\n",
      "Test Episode: 21/30, Score: 999\n",
      "Test Episode: 22/30, Score: 999\n",
      "Test Episode: 23/30, Score: 999\n",
      "Test Episode: 24/30, Score: 999\n",
      "Test Episode: 25/30, Score: 999\n",
      "Test Episode: 26/30, Score: 999\n",
      "Test Episode: 27/30, Score: 999\n",
      "Test Episode: 28/30, Score: 999\n",
      "Test Episode: 29/30, Score: 445\n",
      "Test Episode: 30/30, Score: 999\n",
      "\n",
      "Score average: 980.53, Sigma: 101.15\n",
      "Average time per step: 0.0002 seconds\n",
      "Reward average: 897.23, Sigma: 92.21\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "env = DMControlWrapper(\"cartpole\", \"balance\")\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(1000): # 1000 steps (half delta t of gym)\n",
    "        action = PPO_model.predict(state)[0][0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        sum_rewards += reward\n",
    "    \n",
    "\n",
    "        if state[1] < 0.978 or done or t == 999 : # cos of angle < cos of 12 degrees--> angle > 12deg or 1000 steps (half delta t of gym) \n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_scores.append(t)\n",
    "            test_rewards.append(sum_rewards)\n",
    "            break\n",
    "\n",
    "test_average = mean(test_scores)\n",
    "test_sigma = stdev(test_scores)\n",
    "reward_average = mean(test_rewards)\n",
    "reward_sigma = stdev(test_rewards)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / total_steps\n",
    "\n",
    "print()\n",
    "print('Score average: {:.2f}, Sigma: {:.2f}'.format(test_average, test_sigma))\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "print('Reward average: {:.2f}, Sigma: {:.2f}'.format(reward_average, reward_sigma))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video video/PPO_sb3_dm_balance.mp4.\n",
      "Moviepy - Writing video video/PPO_sb3_dm_balance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video/PPO_sb3_dm_balance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#create video\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "def ppo_policy(time_step):\n",
    "    timestep, reward, discount, observation = time_step\n",
    "    state = np.concatenate((observation['position'],observation['velocity']))\n",
    "    action = PPO_model.predict(state)[0][0]\n",
    "    return action\n",
    "\n",
    "# Load the cartpole environment\n",
    "env = suite.load(domain_name='cartpole', task_name='balance')\n",
    "\n",
    "# Visualization and video creation\n",
    "def save_video(policy):\n",
    "    frames = []\n",
    "\n",
    "    def policy_with_frame_grab(time_step):\n",
    "        pixels = env.physics.render(height=480, width=640, camera_id=0)\n",
    "        frames.append(pixels)\n",
    "        return policy(time_step)\n",
    "\n",
    "    # Create the viewer application\n",
    "    viewer.launch(env, policy=policy_with_frame_grab)\n",
    "\n",
    "    # Save the frames as a video\n",
    "    clip = ImageSequenceClip(frames, fps=100)\n",
    "    clip.write_videofile(\"video/PPO_sb3_dm_balance.mp4\", codec=\"libx264\")\n",
    "\n",
    "# Call the save_video function with your policy function\n",
    "save_video(ppo_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of PPO model trained for balance how it works in swingup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average time per step: 0.0002 seconds\n",
      "Reward average: 323.45, Sigma: 96.18\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "env = DMControlWrapper(\"cartpole\", \"swingup\")\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "test_rewards = []\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    sum_rewards = 0\n",
    "\n",
    "    for t in range(1000): # 1000 steps (half delta t of gym)\n",
    "        action = PPO_model.predict(state)[0][0]\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        \n",
    "        sum_rewards += reward\n",
    "    \n",
    "    test_rewards.append(sum_rewards)\n",
    "\n",
    "\n",
    "reward_average = mean(test_rewards)\n",
    "reward_sigma = stdev(test_rewards)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / (test_episodes * 1000)\n",
    "\n",
    "\n",
    "print()\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "print('Reward average: {:.2f}, Sigma: {:.2f}'.format(reward_average, reward_sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize\n",
    "def PPO_policy(time_step):\n",
    "    timestep, reward, discount, observation = time_step\n",
    "    state = np.concatenate((observation['position'],observation['velocity']))\n",
    "    action = PPO_model.predict(state)[0][0]\n",
    "    return action\n",
    "\n",
    "env = suite.load(domain_name='cartpole', task_name='swingup')\n",
    "\n",
    "viewer.launch(env, policy=PPO_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dqn_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
