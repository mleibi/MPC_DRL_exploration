{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the gym version of the cartpole to start with Imitation Learning\n",
    "\n",
    "gym.openai.com/envs/Cartpole-v1\n",
    "\n",
    "https://www.gymlibrary.dev/environments/classic_control/cart_pole/\n",
    "\n",
    "https://www.youtube.com/watch?v=3zeg7H6cAJw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from statistics import mean, median, stdev\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "env = gym.make('CartPole-v1', render_mode=\"human\")\n",
    "env.reset()\n",
    "goal_steps = 500\n",
    "score_requirement = 50 # all random games have a score of 50 or better\n",
    "initial_games = 10000\n",
    "\n",
    "\n",
    "def some_random_games_first(): # just to see what the game looks like\n",
    "    for episode in range(5):\n",
    "        env.reset()\n",
    "        for t in range(goal_steps):\n",
    "            env.render() # slows down the speed of the game\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info, _ = env.step(action)\n",
    "            if done:\n",
    "                break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Playing some games first with random actions and picking the best ones (score requirement above 50). Then imitate these actions with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accepted score: 62.06849315068493\n",
      "Median accepted score: 58.0\n",
      "Counter({51.0: 28, 55.0: 27, 50.0: 24, 52.0: 22, 56.0: 21, 61.0: 19, 53.0: 19, 59.0: 15, 57.0: 15, 58.0: 14, 54.0: 14, 65.0: 12, 64.0: 12, 68.0: 11, 60.0: 11, 66.0: 9, 69.0: 8, 62.0: 7, 67.0: 7, 63.0: 6, 73.0: 6, 76.0: 5, 74.0: 4, 93.0: 3, 72.0: 3, 89.0: 3, 70.0: 3, 75.0: 3, 95.0: 2, 97.0: 2, 85.0: 2, 105.0: 2, 79.0: 2, 77.0: 2, 88.0: 2, 86.0: 2, 91.0: 2, 71.0: 2, 107.0: 1, 84.0: 1, 87.0: 1, 100.0: 1, 83.0: 1, 81.0: 1, 115.0: 1, 82.0: 1, 102.0: 1, 92.0: 1, 111.0: 1, 90.0: 1, 94.0: 1, 78.0: 1})\n"
     ]
    }
   ],
   "source": [
    "def initial_population():\n",
    "    training_data = []\n",
    "    scores = []\n",
    "    accepted_scores = []\n",
    "\n",
    "    for _ in range(initial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        prev_observation = []\n",
    "\n",
    "        for _ in range(goal_steps):\n",
    "            action = env.action_space.sample()\n",
    "            observation, reward, done, info, _ = env.step(action)\n",
    "\n",
    "            if len(prev_observation) > 0:\n",
    "                game_memory.append([prev_observation, action])\n",
    "            \n",
    "            prev_observation = observation\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory: # convert to one-hot output\n",
    "\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0]\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "    #training_data_save = np.array(training_data)\n",
    "    #np.save('saved.npy', training_data_save)\n",
    "\n",
    "    print('Average accepted score:', mean(accepted_scores))\n",
    "    print('Median accepted score:', median(accepted_scores))\n",
    "    print(Counter(accepted_scores))\n",
    "\n",
    "    return training_data\n",
    "\n",
    "env.close()\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "training_data = initial_population()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "def build_model(state_size, action_size, lr=0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(96, activation='relu'))\n",
    "    model.add(Dense(48, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    #model.compile(loss='mse', optimizer=Adam(learning_rate=lr))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def train_model(training_data, state_size, action_size, model=False):\n",
    "    X = np.array([i[0] for i in training_data]).reshape(-1, state_size)\n",
    "    y = np.array([i[1] for i in training_data]).reshape(-1, action_size) \n",
    "    print(training_data[0][0])\n",
    "    print(X[0])\n",
    "    print(training_data[0][1])\n",
    "    print(y[0])\n",
    "\n",
    "    if not model:\n",
    "        model = build_model()\n",
    "\n",
    "    model.fit(X, y, epochs=5, verbose=1)\n",
    "    return model\n",
    "\n",
    "model = build_model(state_size, action_size, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/10, Score: 10\n",
      "Test Episode: 2/10, Score: 10\n",
      "Test Episode: 3/10, Score: 9\n",
      "Test Episode: 4/10, Score: 10\n",
      "Test Episode: 5/10, Score: 10\n",
      "Test Episode: 6/10, Score: 10\n",
      "Test Episode: 7/10, Score: 11\n",
      "Test Episode: 8/10, Score: 12\n",
      "Test Episode: 9/10, Score: 28\n",
      "Test Episode: 10/10, Score: 22\n",
      "\n",
      "pretraining: score average  13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:00:25.986147: W tensorflow/c/c_api.cc:291] Operation '{name:'dense_61/bias/Assign' id:5378 op device:{requested: '', assigned: ''} def:{{{node dense_61/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_61/bias, dense_61/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "# pretraining test\n",
    "env.close()\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n",
    "test_episodes = 10\n",
    "test_average = 0\n",
    "for e in range(test_episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state[0], [1, state_size])\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "        #env.render()  # Render the environment to visualize the agent's behavior\n",
    "        action = np.argmax(model.predict(state, verbose = 0)[0])\n",
    "        next_state, reward, done, info, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        state = next_state\n",
    "        t += 1\n",
    "        if done:\n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_average += t\n",
    "            break\n",
    "test_average/=test_episodes\n",
    "print()\n",
    "print('pretraining: score average ', test_average)\n",
    "test_average = 0\n",
    "env.close() # finish the rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.02687128 -0.15812276  0.0300712   0.25525638]\n",
      "[-0.02687128 -0.15812276  0.0300712   0.25525638]\n",
      "[1, 0]\n",
      "[1 0]\n",
      "Train on 22290 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:00:26.259315: W tensorflow/c/c_api.cc:291] Operation '{name:'loss_10/mul' id:5514 op device:{requested: '', assigned: ''} def:{{{node loss_10/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_10/mul/x, loss_10/dense_65_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-05-08 11:00:26.340127: W tensorflow/c/c_api.cc:291] Operation '{name:'training_20/Adam/dense_60/bias/v/Assign' id:5726 op device:{requested: '', assigned: ''} def:{{{node training_20/Adam/dense_60/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](training_20/Adam/dense_60/bias/v, training_20/Adam/dense_60/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22290/22290 [==============================] - 1s 24us/sample - loss: 0.2460 - acc: 0.5876\n",
      "Epoch 2/5\n",
      "22290/22290 [==============================] - 0s 21us/sample - loss: 0.2353 - acc: 0.5999\n",
      "Epoch 3/5\n",
      "22290/22290 [==============================] - 0s 21us/sample - loss: 0.2342 - acc: 0.6026\n",
      "Epoch 4/5\n",
      "22290/22290 [==============================] - 0s 22us/sample - loss: 0.2340 - acc: 0.6025\n",
      "Epoch 5/5\n",
      "22290/22290 [==============================] - 1s 23us/sample - loss: 0.2335 - acc: 0.6042\n"
     ]
    }
   ],
   "source": [
    "model = train_model(training_data, state_size, action_size, model) # fast training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/30, Score: 110\n",
      "Test Episode: 2/30, Score: 121\n",
      "Test Episode: 3/30, Score: 206\n",
      "Test Episode: 4/30, Score: 202\n",
      "Test Episode: 5/30, Score: 215\n",
      "Test Episode: 6/30, Score: 148\n",
      "Test Episode: 7/30, Score: 421\n",
      "Test Episode: 8/30, Score: 145\n",
      "Test Episode: 9/30, Score: 374\n",
      "Test Episode: 10/30, Score: 181\n",
      "Test Episode: 11/30, Score: 147\n",
      "Test Episode: 12/30, Score: 308\n",
      "Test Episode: 13/30, Score: 247\n",
      "Test Episode: 14/30, Score: 123\n",
      "Test Episode: 15/30, Score: 154\n",
      "Test Episode: 16/30, Score: 87\n",
      "Test Episode: 17/30, Score: 289\n",
      "Test Episode: 18/30, Score: 161\n",
      "Test Episode: 19/30, Score: 95\n",
      "Test Episode: 20/30, Score: 120\n",
      "Test Episode: 21/30, Score: 130\n",
      "Test Episode: 22/30, Score: 182\n",
      "Test Episode: 23/30, Score: 119\n",
      "Test Episode: 24/30, Score: 198\n",
      "Test Episode: 25/30, Score: 95\n",
      "Test Episode: 26/30, Score: 94\n",
      "Test Episode: 27/30, Score: 362\n",
      "Test Episode: 28/30, Score: 280\n",
      "Test Episode: 29/30, Score: 116\n",
      "Test Episode: 30/30, Score: 108\n",
      "\n",
      "Score average: 184.60, Sigma: 90.83\n",
      "Average time per step: 0.0004 seconds\n"
     ]
    }
   ],
   "source": [
    "# postraining test\n",
    "env = gym.make('CartPole-v1') # no visualization\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    state, done = env.reset()\n",
    "\n",
    "    for t in range(501):\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        action = np.argmax(model.predict(state, verbose = 0)[0])\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "    \n",
    "\n",
    "        if done or t == 500:\n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_scores.append(t)\n",
    "            break\n",
    "\n",
    "test_average = mean(test_scores)\n",
    "test_sigma = stdev(test_scores)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / total_steps\n",
    "\n",
    "print()\n",
    "print('Score average: {:.2f}, Sigma: {:.2f}'.format(test_average, test_sigma))\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game  0 score:  265.0\n",
      "Average accepted score: 380.6666666666667\n",
      "Median accepted score: 369.5\n",
      "Counter({448.0: 1, 407.0: 1, 475.0: 1, 332.0: 1, 302.0: 1, 320.0: 1})\n",
      "[265.0, 103.0, 189.0, 78.0, 179.0, 243.0, 171.0, 101.0, 166.0, 112.0, 127.0, 83.0, 85.0, 117.0, 243.0, 86.0, 229.0, 96.0, 122.0, 133.0, 89.0, 79.0, 183.0, 179.0, 221.0, 103.0, 117.0, 173.0, 448.0, 110.0, 166.0, 80.0, 179.0, 147.0, 221.0, 148.0, 191.0, 117.0, 87.0, 91.0, 187.0, 166.0, 172.0, 157.0, 135.0, 207.0, 120.0, 136.0, 192.0, 86.0, 94.0, 76.0, 145.0, 128.0, 89.0, 279.0, 204.0, 186.0, 165.0, 107.0, 125.0, 290.0, 147.0, 108.0, 196.0, 234.0, 407.0, 124.0, 118.0, 121.0, 156.0, 76.0, 99.0, 86.0, 475.0, 264.0, 332.0, 193.0, 97.0, 130.0, 234.0, 125.0, 302.0, 101.0, 157.0, 141.0, 273.0, 189.0, 129.0, 239.0, 91.0, 183.0, 130.0, 107.0, 320.0, 158.0, 156.0, 81.0, 225.0, 92.0]\n"
     ]
    }
   ],
   "source": [
    "initial_games = 100 # trying to get improved data\n",
    "score_requirement = 300\n",
    "\n",
    "def improved_data():\n",
    "    training_data = []\n",
    "    scores = []\n",
    "    accepted_scores = []\n",
    "\n",
    "    for g in range(initial_games):\n",
    "        score = 0\n",
    "        game_memory = []\n",
    "        prev_observation = []\n",
    "\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state[0], [1, state_size])\n",
    "\n",
    "        for _ in range(goal_steps):\n",
    "            action = np.argmax(model.predict(state, verbose = 0)[0])\n",
    "            observation, reward, done, info, _ = env.step(action)\n",
    "            state = np.reshape(observation, [1, state_size])\n",
    "\n",
    "            if len(prev_observation) > 0:\n",
    "                game_memory.append([prev_observation, action])\n",
    "            \n",
    "            prev_observation = observation\n",
    "            score += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if score >= score_requirement:\n",
    "            accepted_scores.append(score)\n",
    "            for data in game_memory: # convert to one-hot output\n",
    "\n",
    "                if data[1] == 1:\n",
    "                    output = [0, 1]\n",
    "                elif data[1] == 0:\n",
    "                    output = [1, 0]\n",
    "                training_data.append([data[0], output])\n",
    "\n",
    "        env.reset()\n",
    "        scores.append(score)\n",
    "\n",
    "        if g % 100 == 0:\n",
    "            print('Game ', g, 'score: ', score)\n",
    "    #training_data_save = np.array(training_data)\n",
    "    #np.save('saved.npy', training_data_save)\n",
    "\n",
    "    if len(accepted_scores) > 0:\n",
    "        print('Average accepted score:', mean(accepted_scores))\n",
    "        print('Median accepted score:', median(accepted_scores))\n",
    "        print(Counter(accepted_scores))\n",
    "    else:\n",
    "        print('No scores above the score requirement')\n",
    "\n",
    "    print(scores)\n",
    "    return training_data\n",
    "\n",
    "env.reset()\n",
    "training_data = improved_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.05051852  0.14719273  0.02287117 -0.26850793]\n",
      "[-0.05051852  0.14719273  0.02287117 -0.26850793]\n",
      "[1, 0]\n",
      "[1 0]\n",
      "Train on 2278 samples\n",
      "Epoch 1/5\n",
      "2278/2278 [==============================] - 0s 26us/sample - loss: 0.1253 - acc: 0.8652\n",
      "Epoch 2/5\n",
      "2278/2278 [==============================] - 0s 23us/sample - loss: 0.0895 - acc: 0.8766\n",
      "Epoch 3/5\n",
      "2278/2278 [==============================] - 0s 25us/sample - loss: 0.0824 - acc: 0.8819\n",
      "Epoch 4/5\n",
      "2278/2278 [==============================] - 0s 25us/sample - loss: 0.0836 - acc: 0.8788\n",
      "Epoch 5/5\n",
      "2278/2278 [==============================] - 0s 28us/sample - loss: 0.0732 - acc: 0.8968\n"
     ]
    }
   ],
   "source": [
    "model = train_model(training_data, state_size, action_size, model) # train again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Episode: 1/30, Score: 174\n",
      "Test Episode: 2/30, Score: 192\n",
      "Test Episode: 3/30, Score: 98\n",
      "Test Episode: 4/30, Score: 138\n",
      "Test Episode: 5/30, Score: 143\n",
      "Test Episode: 6/30, Score: 175\n",
      "Test Episode: 7/30, Score: 155\n",
      "Test Episode: 8/30, Score: 80\n",
      "Test Episode: 9/30, Score: 164\n",
      "Test Episode: 10/30, Score: 74\n",
      "Test Episode: 11/30, Score: 249\n",
      "Test Episode: 12/30, Score: 107\n",
      "Test Episode: 13/30, Score: 247\n",
      "Test Episode: 14/30, Score: 75\n",
      "Test Episode: 15/30, Score: 101\n",
      "Test Episode: 16/30, Score: 151\n",
      "Test Episode: 17/30, Score: 86\n",
      "Test Episode: 18/30, Score: 142\n",
      "Test Episode: 19/30, Score: 148\n",
      "Test Episode: 20/30, Score: 500\n",
      "Test Episode: 21/30, Score: 85\n",
      "Test Episode: 22/30, Score: 94\n",
      "Test Episode: 23/30, Score: 94\n",
      "Test Episode: 24/30, Score: 172\n",
      "Test Episode: 25/30, Score: 151\n",
      "Test Episode: 26/30, Score: 120\n",
      "Test Episode: 27/30, Score: 84\n",
      "Test Episode: 28/30, Score: 500\n",
      "Test Episode: 29/30, Score: 386\n",
      "Test Episode: 30/30, Score: 128\n",
      "\n",
      "Score average: 167.10, Sigma: 111.21\n",
      "Average time per step: 0.0004 seconds\n"
     ]
    }
   ],
   "source": [
    "# postraining 2 test\n",
    "\n",
    "env = gym.make('CartPole-v1') # no visualization\n",
    "\n",
    "# Test\n",
    "test_episodes = 30\n",
    "test_scores = []\n",
    "start_time = time.time()\n",
    "\n",
    "for e in range(test_episodes):\n",
    "    state, done = env.reset()\n",
    "\n",
    "    for t in range(501):\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        action = np.argmax(model.predict(state, verbose = 0)[0])\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "    \n",
    "\n",
    "        if done or t == 500:\n",
    "            print(\"Test Episode: {}/{}, Score: {}\".format(e + 1, test_episodes, t))\n",
    "            test_scores.append(t)\n",
    "            break\n",
    "\n",
    "test_average = mean(test_scores)\n",
    "test_sigma = stdev(test_scores)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "total_steps = sum(test_scores)\n",
    "average_time_per_step = total_time / total_steps\n",
    "\n",
    "print()\n",
    "print('Score average: {:.2f}, Sigma: {:.2f}'.format(test_average, test_sigma))\n",
    "print('Average time per step: {:.4f} seconds'.format(average_time_per_step))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video video/ML_gym_balance.mp4.\n",
      "Moviepy - Writing video video/ML_gym_balance.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video/ML_gym_balance.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import ImageSequenceClip\n",
    "\n",
    "# Load the cartpole environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Visualization and video creation\n",
    "def save_video():\n",
    "    frames = []\n",
    "\n",
    "    state, done = env.reset()\n",
    "    for t in range(501):\n",
    "        pixels = env.render()\n",
    "        frames.append(pixels)\n",
    "\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        action = np.argmax(model.predict(state, verbose = 0)[0])\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "\n",
    "        if done or t == 500:\n",
    "            break\n",
    "\n",
    "    # Save the frames as a video\n",
    "    clip = ImageSequenceClip(frames, fps=50)\n",
    "    clip.write_videofile(\"video/ML_gym_balance.mp4\", codec=\"libx264\")\n",
    "\n",
    "# Call the save_video function with your policy function\n",
    "save_video()\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
